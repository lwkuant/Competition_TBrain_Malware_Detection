# -*- coding: utf-8 -*-
"""
Modeling
"""

### Load Packages
import pandas as pd
import numpy as np
from scipy import stats
from time import time
import re
import pickle
seed_split = 123
seed_feature = 1000
seed_skf = 2000
seed_model = 3000
seed_skf_layer_2 = 4000

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegressionCV

from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score

#import tensorflow as tf
#from keras import backend as K  

import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

import os
os.chdir('D:/Google雲端硬碟/Project/Competition_TBrain_Malware_Detection/Datasets')


### Load the Data
df_train = pd.read_csv('df_train.csv')
#df_train = df_train.ix[:, :-6]
 
### Split the data
df_tr, df_valid = train_test_split(df_train, test_size = 0.2, random_state = seed_split, shuffle = True, stratify = df_train['Target'])
df_tr.index = range(df_tr.shape[0])
df_valid.index = range(df_valid.shape[0])

#np.where(np.array([np.std(x) for x in df_tr.ix[:, 3:].values.transpose()]) == 0)
#df_tr.columns[61]


### EDA

## Label Weight 
df_tr['Target'].value_counts()/df_tr.shape[0]

pos_ratio = (df_tr['Target'].value_counts()/df_tr.shape[0])[1]
neg_ratio = (df_tr['Target'].value_counts()/df_tr.shape[0])[0]

[feature for feature in df_tr.columns[3:]]

#plot_list = ['Occur_count', 'Occur_count_distinct_customer', 'Occur_count_distinct_product']
#plt.figure(figsize = [20, 15])
#sns.heatmap(df_tr.ix[:, plot_list].corr(), cmap = 'RdBu_r', vmin = -1, vmax = 1, annot = True)
#plt.xticks(rotation=30)
#plt.savefig('corr_plot.png')

### Construct Features

## Training Set
df_tr['Mean_occur_count_per_customer'] = df_tr['Occur_count']/df_tr['Occur_count_distinct_customer']
#
df_tr['Mean_occur_count_per_product'] = df_tr['Occur_count']/df_tr['Occur_count_distinct_product']
#
df_tr['time_interval_occurence_cv'] = df_tr.ix[:, ['time_interval_occurence_std', 'time_interval_occurence_mean']].apply(lambda x: (x[0]/x[1] if x[1] != 0 else 0), axis = 1)
#
df_tr['Weekday_weekend_norm'] = df_tr['Weekday_6_norm'] + df_tr['Weekday_7_norm']
df_tr['Weekday_non_weekend_norm'] = df_tr['Weekday_1_norm'] + df_tr['Weekday_2_norm'] + df_tr['Weekday_3_norm'] + df_tr['Weekday_4_norm'] + df_tr['Weekday_5_norm']
# 
#df_tr['Mean_occur_count_per_customer_log'] = np.log1p(df_tr['Mean_occur_count_per_customer'])
#
#df_tr['Mean_occur_count_per_product_log'] = np.log1p(df_tr['Mean_occur_count_per_product'])
#
df_tmp = df_tr.ix[:, 50:79].apply(lambda x: x/np.sum(x), axis = 1)
df_tmp.columns = [x + '_norm' for x in df_tr.columns[50:79]]
df_tr = pd.concat([df_tr, df_tmp], axis = 1)
#
df_tr['QueryTS_cv'] = df_tr['QueryTS_std']/df_tr['QueryTS_mean'] 
#
df_tr['QueryTS_variance'] = df_tr['QueryTS_std']**2 
#
df_tr['Occur_count_log'] = np.log(df_tr['Occur_count'])
#
df_tr['QueryTS_mean_mdeian_diff'] = df_tr['QueryTS_mean'] - df_tr['QueryTS_median']
#
df_tr['time_interval_occurence_variance'] = df_tr['time_interval_occurence_std']**2
#
df_tr['Mean_occur_count_per_product_to_customer'] = (df_tr['Mean_occur_count_per_product'].values+1)/(df_tr['Mean_occur_count_per_customer'].values+1)
#
df_tr['time_interval_occurence_mean_mdeian_diff'] = df_tr['time_interval_occurence_mean'] - df_tr['time_interval_occurence_median']
#

## Validation Set
df_valid['Mean_occur_count_per_customer'] = df_valid['Occur_count']/df_valid['Occur_count_distinct_customer']
#
df_valid['Mean_occur_count_per_product'] = df_valid['Occur_count']/df_valid['Occur_count_distinct_product']
#
df_valid['time_interval_occurence_cv'] = df_valid.ix[:, ['time_interval_occurence_std', 'time_interval_occurence_mean']].apply(lambda x: (x[0]/x[1] if x[1] != 0 else 0), axis = 1)
#
df_valid['Weekday_weekend_norm'] = df_valid['Weekday_6_norm'] + df_valid['Weekday_7_norm']
df_valid['Weekday_non_weekend_norm'] = df_valid['Weekday_1_norm'] + df_valid['Weekday_2_norm'] + df_valid['Weekday_3_norm'] + df_valid['Weekday_4_norm'] + df_valid['Weekday_5_norm']
#
df_tmp = df_valid.ix[:, 50:79].apply(lambda x: x/np.sum(x), axis = 1)
df_tmp.columns = [x + '_norm' for x in df_valid.columns[50:79]]
df_valid = pd.concat([df_valid, df_tmp], axis = 1)
#
df_valid['QueryTS_cv'] = df_valid['QueryTS_std']/df_valid['QueryTS_mean'] 
#
df_valid['QueryTS_variance'] = df_valid['QueryTS_std']**2 
#
df_valid['Occur_count_log'] = np.log(df_valid['Occur_count'])
#
df_valid['QueryTS_mean_mdeian_diff'] = df_valid['QueryTS_mean'] - df_valid['QueryTS_median']
#
df_valid['time_interval_occurence_variance'] = df_valid['time_interval_occurence_std']**2
#
df_valid['Mean_occur_count_per_product_to_customer'] = (df_valid['Mean_occur_count_per_product'].values+1)/(df_valid['Mean_occur_count_per_customer'].values+1)
#
df_valid['time_interval_occurence_mean_mdeian_diff'] = df_valid['time_interval_occurence_mean'] - df_valid['time_interval_occurence_median']
#


feature_to_use = open('Feature_to_use_v2')
feature_to_use = feature_to_use.read()
feature_to_use = re.sub('\n', '', feature_to_use)
feature_to_use = eval(feature_to_use)


feature_to_remove = open('feature_to_remove')
feature_to_remove = feature_to_remove.read()
feature_to_remove = re.sub('\n', '', feature_to_remove)
feature_to_remove = eval(feature_to_remove)

y_valid = df_valid['Target'].values

df_valid = df_valid.ix[:, [x for x in df_valid.columns if x not in feature_to_remove]]

df_valid = df_valid.ix[:, feature_to_use]
### Preprocess the data (Normalization)
#std_scaler = StandardScaler()
#std_scaler.fit(df_tr.ix[:, 3:])

#X_train = std_scaler.transform(df_tr.ix[:, 3:])
#y_train = df_tr['Target'].values

#X_valid = std_scaler.transform(df_valid.ix[:, 3:])
#y_valid = df_valid['Target'].values

#feature_to_remove = open('feature_to_remove')
#feature_to_remove = feature_to_remove.read()
#feature_to_remove = re.sub('\n', '', feature_to_remove)
#feature_to_remove = eval(feature_to_remove)

y_train = df_tr['Target'].values

df_tr = df_tr.ix[:, [x for x in df_tr.columns if x not in feature_to_remove]]

df_tr = df_tr.ix[:, feature_to_use]
### Preprocess the data (Without Normalization)

#X_train = df_tr.ix[:, 3:].values
#X_valid = df_valid.ix[:, 3:].values

#X_valid = df_valid.ix[:, 3:].values
#y_valid = df_valid['Target'].values

X_train = df_tr.ix[:, :].values
X_valid = df_valid.ix[:, :].values


def model_training_feature_importance(model, df, x, y):
    start_time = time()
    model.fit(x, y)
    print(time() - start_time)
    
    feature_importance = sorted(list(zip(list(df.columns)[3:], 
               model.feature_importances_)), key = lambda x: x[1]*(-1))
    
    score = roc_auc_score(y, model.predict_proba(x)[:, 1])
    
    return [feature_importance, score]

###

# Random Forest
model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_feature,
                                  n_jobs = -1,
                                  class_weight = {0: neg_ratio, 1:pos_ratio})
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
rf_feature_importance = outcome[0]
print(outcome[1])

# XGBOOST
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1,
                          class_weight = {0: neg_ratio, 1:pos_ratio})
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
xgb_feature_importance = outcome[0]
print(outcome[1])

# LGBM

model = LGBMClassifier(n_estimators = 300,
                          random_state = seed_feature,
                          n_jobs = -1)
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
lgbm_feature_importance = outcome[0]
print(outcome[1])

# Adaboost
model = AdaBoostClassifier(n_estimators = 300,
                          random_state = seed_feature)
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
ada_feature_importance = outcome[0]
print(outcome[1])

# Gradient Boosting
model = GradientBoostingClassifier(n_estimators = 300,
                          random_state = seed_feature)
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
gradient_feature_importance = outcome[0]
print(outcome[1])

# ExtraTreesClassifier
model = ExtraTreesClassifier(n_estimators = 300,
                          random_state = seed_feature,
                          n_jobs = -1,
                          class_weight = {0: neg_ratio, 1:pos_ratio})
outcome = model_training_feature_importance(model, df_tr, X_train, y_train)
extratrees_feature_importance = outcome[0]
print(outcome[1])

def feature_rank(feature_importance_list, feature_list):
    feature_importance_list = [[x[0] for x in y] for y in feature_importance_list]
    return sorted([(x, np.mean([y.index(x) for y in feature_importance_list])) for x in feature_list], key = lambda x: x[1])

feature_importance_list = [rf_feature_importance,
                           xgb_feature_importance,
                           lgbm_feature_importance,
                           ada_feature_importance,
                           gradient_feature_importance,
                           extratrees_feature_importance]

feature_rank(feature_importance_list, df_tr.columns[3:])
#feature_list = [x[0] for x in feature_rank(feature_importance_list, df_tr.columns[3:])]
feature_list = feature_to_use

df_tr['Target'] = y_train

def K_fold_validation(model, n_kfold, df, feature_to_use, seed_skf):
    skf = StratifiedKFold(n_splits=n_kfold, shuffle=True, random_state=seed_skf)
    
    start_time = time()
    score_list = []
    
    for train_ind, test_ind in list(skf.split(df.ix[:, feature_to_use].values, df['Target'].values)):
        model.fit(df.ix[:, feature_to_use].values[train_ind, :], df['Target'].values[train_ind])
        score = roc_auc_score(df['Target'].values[test_ind],
                                        model.predict_proba(df.ix[:, feature_to_use].values[test_ind, :])[:, 1])
        score_list.append(score)
        print(score)
    done_time = time() - start_time
    print(done_time)
    
    return np.mean(score_list)    

model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
K_fold_validation(model, 10, df_tr, feature_list, seed_skf) # 0.94451599586106494
# 256.0648024082184s

model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
K_fold_validation(model, 10, df_tr, feature_list, seed_skf) # 0.93312719100735664
# 590.272463798523s


lgbm_feature_to_use = ['QueryTS_mean_mdeian_diff',
 'time_interval_occurence_Q1',
 'ProductID_7acab3_norm',
 'time_interval_occurence_median',
 'ProductID_3ea8c3_norm',
 'Mean_occur_count_per_customer',
 'QueryTS_range',
 'Num_of_product_type_customer_cnt_1_norm',
 'time_interval_occurence_cv',
 'Hour_lvl_3_3_norm',
 'Mean_occur_count_per_product_to_customer',
 'time_interval_occurence_Q3',
 'QueryTS_min',
 'Hour_lvl_3_2_norm',
 'Hour_lvl_3_1_norm',
 'time_interval_occurence_IQR',
 'ProductID_055649_norm',
 'time_interval_occurence_skew',
 'QueryTS_std',
 'ProductID_634e6b_norm',
 'Mean_occur_count_per_product',
 'ProductID_c105a0_norm',
 'ProductID_3ea8c3',
 'Hour_lvl_2_0_norm',
 'QueryTS_mean',
 'ProductID_20f8a5',
 'time_interval_occurence_max',
 'Occur_count',
 'ProductID_7acab3',
 'ProductID_20f8a5_norm',
 'time_interval_occurence_std',
 'ProductID_055649',
 'time_interval_occurence_kurtosis',
 'ProductID_c76d58_norm',
 'Weekday_weekend_norm',
 'QueryTS_median',
 'Hour_lvl_2_1_norm',
 'Occur_count_distinct_customer',
 'Num_of_product_type_customer_cnt_3',
 'Num_of_product_type_customer_cnt_2',
 'time_interval_occurence_mean_mdeian_diff',
 'Weekday_non_weekend_norm',
 'Occur_count_distinct_product',
 'Occur_count_normalized_by_time',
 'ProductID_d465fc_norm',
 'time_interval_occurence_mean',
 'ProductID_634e6b',
 'ProductID_c105a0',
 'Day_lvl_3_3_norm',
 'Day_lvl_3_1_norm',
 'Day_lvl_3_2_norm',
 'QueryTS_cv',
 'ProductID_c76d58',
 'ProductID_b93794_norm',
 'ProductID_e47f04_norm',
 'ProductID_885fab_norm',
 'Num_of_product_type_customer_cnt_1',
 'time_interval_occurence_range',
 'ProductID_533133_norm',
 'ProductID_e47f04',
 'time_interval_occurence_min',
 'ProductID_885fab',
 'ProductID_d465fc',
 'ProductID_218578_norm',
 'ProductID_b93794',
 'Num_of_product_type_customer_cnt_7',
 'ProductID_8541a0_norm',
 'ProductID_dd8d4a',
 'ProductID_533133',
 'ProductID_dd8d4a_norm',
 'ProductID_26a5d0',
 'Num_of_product_type_customer_cnt_4',
 'ProductID_26a5d0_norm',
 'Occur_count_log',
 'ProductID_a310bb_norm',
 'ProductID_262880',
 'ProductID_aaa9c8',
 'ProductID_75f310',
 'ProductID_8452da',
 'ProductID_05b409',
 'ProductID_a310bb',
 'ProductID_3c2be6',
 'ProductID_8b7f69',
 'ProductID_0cdb7a',
 'ProductID_fec24f',
 'ProductID_218578',
 'ProductID_8541a0',
 'ProductID_0374c4',
 'ProductID_cc3a6a',
 'ProductID_262880_norm',
 'ProductID_aaa9c8_norm',
 'ProductID_75f310_norm',
 'ProductID_8452da_norm',
 'ProductID_05b409_norm',
 'ProductID_3c2be6_norm',
 'ProductID_8b7f69_norm',
 'ProductID_0cdb7a_norm',
 'ProductID_fec24f_norm',
 'ProductID_0374c4_norm',
 'ProductID_cc3a6a_norm',
 'QueryTS_variance',
 'time_interval_occurence_variance']

model = LGBMClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
K_fold_validation(model, 10, df_tr, lgbm_feature_to_use, seed_skf) # 0.94868263915693429
# 62.6761634349823s

model = AdaBoostClassifier(n_estimators = 300,
                          random_state = seed_model)
K_fold_validation(model, 10, df_tr, feature_list, seed_skf) # 0.90421646818042078
# 809.0872654914856

model = GradientBoostingClassifier(n_estimators = 300,
                          random_state = seed_model)
K_fold_validation(model, 10, df_tr, feature_list, seed_skf) # 0.92852779322525758
# 768.2441251277924

model = ExtraTreesClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
K_fold_validation(model, 10, df_tr, feature_list, seed_skf) # 0.94338633554226536
# 86.58322787284851s




index_list = []
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_list].values, df_tr['Target'].values)):
    print(test_ind[:10])
    index_list.append(test_ind)

index_list = list(np.concatenate(index_list))
seq_ind = list(range(df_tr.shape[0]))
correct_ind = [index_list.index(x) for x in seq_ind]


def K_fold_train_prediction(model, df_train, df_test, seed_skf, num_fold, feature_to_use, correct_ind):
    
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=seed_skf)
    
    train_pred_list = []
    test_pred_list = []
    index_list = []
    model_list = []
    for train_ind, test_ind in list(skf.split(df_train.ix[:, feature_to_use].values, df_train['Target'].values)):
        start_time = time()
        model.fit(df_train.ix[:, feature_to_use].values[train_ind, :], df_train['Target'].values[train_ind])
        train_pred_list.append(model.predict_proba(df_train.ix[:, feature_to_use].values[test_ind, :])[:, 1])
        test_pred_list.append(model.predict_proba(df_test.ix[:, feature_to_use].values)[:, 1])
        index_list.append(test_ind)
        model_list.append(model)
        print(test_ind[:10])
        print(time() - start_time)
    
    index_list = list(np.concatenate(index_list))
    #seq_ind = list(range(df_train.shape[0]))
    #correct_ind = [index_list.index(x) for x in seq_ind]
    train_pred_list = np.concatenate(train_pred_list)[correct_ind]
    
    return [train_pred_list,
            np.mean(np.c_[test_pred_list].transpose(), axis = 1),
            model_list,
            index_list]
    

### Random Forest
model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
rf_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)

### Xgboost
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
xgb_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)

### LGBM
model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1)
lgbm_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)

### Adaboost
model = AdaBoostClassifier(n_estimators = 300,
                          random_state = seed_model)
ada_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)

### Gradient boosting
model = GradientBoostingClassifier(n_estimators = 300,
                          random_state = seed_model)
gradient_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)

### Extra Trees
model = ExtraTreesClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
extratrees_outcome = K_fold_train_prediction(model, df_tr, df_valid, seed_skf, 10, feature_list, correct_ind)




###

model_train_outcome_list = [rf_outcome[0],
                            xgb_outcome[0],
                            lgbm_outcome[0],
                            ada_outcome[0],
                            gradient_outcome[0],
                            extratrees_outcome[0]]
model_train_name_list = ['Random_Forest_v1',
                         'Xbgoost_v1',
                         'LGBM_v1',
                         'Adaboost_v1',
                         'Gradient_Boosting_v1',
                         'Extra_Trees_v1']

model_valid_outcome_list = [rf_outcome[1],
                            xgb_outcome[1],
                            lgbm_outcome[1],
                            ada_outcome[1],
                            gradient_outcome[1],
                            extratrees_outcome[1]]

np.c_[model_train_outcome_list].transpose()

pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list).corr()

X_train_layer_2_df  = pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list)
X_train_layer_2_df['Target'] = y_train

X_train_layer_2 = np.c_[model_train_outcome_list].transpose()
X_valid_layer_2 = np.c_[model_valid_outcome_list].transpose()

### 
model = LogisticRegression() 
# 0.94933298189212034

model = LogisticRegressionCV()
# 0.94979676084861675

model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1) 
# 0.95089391184438732

model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
#  0.94553759044747332

model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1)
# 0.94786385014570507

K_fold_validation(model, 10, X_train_layer_2_df, model_train_name_list, seed_skf)


model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1)
model = LogisticRegression()
model = LogisticRegressionCV(random_state = seed_model)
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1) 
model.fit(X_train_layer_2, y_train)
model.feature_importances_

roc_auc_score(y_train, model.predict_proba(X_train_layer_2)[:, 1])
roc_auc_score(y_valid, model.predict_proba(X_valid_layer_2)[:, 1])



### Prediction on the test set
df_train['Mean_occur_count_per_customer'] = df_train['Occur_count']/df_train['Occur_count_distinct_customer']
df_train['Mean_occur_count_per_product'] = df_train['Occur_count']/df_train['Occur_count_distinct_product']
df_train['time_interval_occurence_cv'] = df_train.ix[:, ['time_interval_occurence_std', 'time_interval_occurence_mean']].apply(lambda x: (x[0]/x[1] if x[1] != 0 else 0), axis = 1)
df_train['Weekday_weekend_norm'] = df_train['Weekday_6_norm'] + df_train['Weekday_7_norm']
df_train['Weekday_non_weekend_norm'] = df_train['Weekday_1_norm'] + df_train['Weekday_2_norm'] + df_train['Weekday_3_norm'] + df_train['Weekday_4_norm'] + df_train['Weekday_5_norm']
df_train['Mean_occur_count_per_customer_log'] = np.log1p(df_train['Mean_occur_count_per_customer'])
df_train['Mean_occur_count_per_product_log'] = np.log1p(df_train['Mean_occur_count_per_product'])
df_tmp = df_train.ix[:, 50:79].apply(lambda x: x/np.sum(x), axis = 1)
df_tmp.columns = [x + '_norm' for x in df_train.columns[50:79]]
df_train = pd.concat([df_train, df_tmp], axis = 1)
df_train['QueryTS_cv'] = df_train['QueryTS_std']/df_train['QueryTS_mean'] 
df_train['QueryTS_variance'] = df_train['QueryTS_std']**2 
df_train['Occur_count_log'] = np.log(df_train['Occur_count'])
df_train['QueryTS_mean_mdeian_diff'] = df_train['QueryTS_mean'] - df_train['QueryTS_median']
df_train['time_interval_occurence_variance'] = df_train['time_interval_occurence_std']**2
df_train['Mean_occur_count_per_product_to_customer'] = (df_train['Mean_occur_count_per_product'].values+1)/(df_train['Mean_occur_count_per_customer'].values+1)
df_train['time_interval_occurence_mean_mdeian_diff'] = df_train['time_interval_occurence_mean'] - df_train['time_interval_occurence_median']

feature_to_remove = open('feature_to_remove')
feature_to_remove = feature_to_remove.read()
feature_to_remove = re.sub('\n', '', feature_to_remove)
feature_to_remove = eval(feature_to_remove)

df_train = df_train.ix[:, [x for x in df_train.columns if x not in feature_to_remove]]
#df_train = df_train.ix[:, feature_list + ['Target']]
df_train = df_train.ix[:, 2:]
feature_to_use = list(df_train.columns[1:])

model = LGBMClassifier(n_estimators = 500,
                          random_state = seed_model,
                          n_jobs = -1)
model = LGBMClassifier(n_estimators=600,learning_rate=0.1, feature_fraction = 0.9, random_state=seed_model, reg_lambda=10)
model = LGBMClassifier(learning_rate = 0.03,
            max_depth = 20,
            scale_pos_weight = 400,
            num_leaves = 30, 
            num_trees = 500, 
            objective = 'binary', 
            lambda_l2 = 100,
            metric = 'auc',
                      random_state = seed_model,
                      n_jobs = -1,
                         n_estimators = 500)
K_fold_validation(model, 10, df_train, feature_to_use, seed_skf) 
# LGBMClassifier(n_estimators=800,learning_rate=0.1, feature_fraction = 0.9, random_state=seed_model, reg_lambda=5,): 0.95428285097487586
# LGBMClassifier(n_estimators=800,learning_rate=0.1, feature_fraction = 0.9, random_state=seed_model, reg_lambda=10,): 0.95480165922512872
# LGBMClassifier(n_estimators=800,learning_rate=0.05, feature_fraction = 0.9, random_state=seed_model, reg_lambda=10,): 0.95425249642713883
# LGBMClassifier(n_estimators=800,learning_rate=0.1, feature_fraction = 0.8, random_state=seed_model, reg_lambda=10,): 0.95441124347739259


model = LogisticRegression()
K_fold_validation(model, 10, df_train, feature_to_use, seed_skf) 

model = ExtraTreesClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
K_fold_validation(model, 10, df_train, feature_to_use, seed_skf) 


model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
K_fold_validation(model, 10, df_train, feature_to_use, seed_skf) 


model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 4, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1)
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
K_fold_validation(model, 10, df_train, feature_to_use, seed_skf) 



################################################################################################################
df_test = pd.read_csv('df_test.csv')

df_test['Mean_occur_count_per_customer'] = df_test['Occur_count']/df_test['Occur_count_distinct_customer']
df_test['Mean_occur_count_per_product'] = df_test['Occur_count']/df_test['Occur_count_distinct_product']
df_test['time_interval_occurence_cv'] = df_test.ix[:, ['time_interval_occurence_std', 'time_interval_occurence_mean']].apply(lambda x: (x[0]/x[1] if x[1] != 0 else 0), axis = 1)
df_test['Weekday_weekend_norm'] = df_test['Weekday_6_norm'] + df_test['Weekday_7_norm']
df_test['Weekday_non_weekend_norm'] = df_test['Weekday_1_norm'] + df_test['Weekday_2_norm'] + df_test['Weekday_3_norm'] + df_test['Weekday_4_norm'] + df_test['Weekday_5_norm']
df_test['Mean_occur_count_per_customer_log'] = np.log1p(df_test['Mean_occur_count_per_customer'])
df_test['Mean_occur_count_per_product_log'] = np.log1p(df_test['Mean_occur_count_per_product'])
df_tmp = df_test.ix[:, 50:79].apply(lambda x: x/np.sum(x), axis = 1)
df_tmp.columns = [x + '_norm' for x in df_test.columns[50:79]]
df_test = pd.concat([df_test, df_tmp], axis = 1)
df_test['QueryTS_cv'] = df_test['QueryTS_std']/df_test['QueryTS_mean'] 
df_test['QueryTS_variance'] = df_test['QueryTS_std']**2 
df_test['Occur_count_log'] = np.log(df_test['Occur_count'])
df_test['QueryTS_mean_mdeian_diff'] = df_test['QueryTS_mean'] - df_test['QueryTS_median']
df_test['time_interval_occurence_variance'] = df_test['time_interval_occurence_std']**2
df_test['Mean_occur_count_per_product_to_customer'] = (df_test['Mean_occur_count_per_product'].values+1)/(df_test['Mean_occur_count_per_customer'].values+1)
df_test['time_interval_occurence_mean_mdeian_diff'] = df_test['time_interval_occurence_mean'] - df_test['time_interval_occurence_median']

#df_test = df_test.ix[:, [x for x in df_test.columns if x not in feature_to_remove]]
df_test = df_test.ix[:, feature_list]

### Training
index_list = []
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)
for train_ind, test_ind in list(skf.split(df_train.ix[:, feature_list].values, df_train['Target'].values)):
    print(test_ind[:10])
    index_list.append(test_ind)

index_list = list(np.concatenate(index_list))
seq_ind = list(range(df_train.shape[0]))
correct_ind = [index_list.index(x) for x in seq_ind]


### Random Forest
model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
rf_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)

### Xgboost
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
xgb_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)

### LGBM
model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1)
lgbm_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)

### Adaboost
model = AdaBoostClassifier(n_estimators = 300,
                          random_state = seed_model)
ada_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)

### Gradient boosting
model = GradientBoostingClassifier(n_estimators = 300,
                          random_state = seed_model)
gradient_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)

### Extra Trees
model = ExtraTreesClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
extratrees_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_list, correct_ind)


### Training and prediction (2018/03/23)

index_list = []
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)
for train_ind, test_ind in list(skf.split(df_train.ix[:, feature_to_use].values, df_train['Target'].values)):
    print(test_ind[:10])
    index_list.append(test_ind)

index_list = list(np.concatenate(index_list))
seq_ind = list(range(df_train.shape[0]))
correct_ind = [index_list.index(x) for x in seq_ind]


# LGBM
model = LGBMClassifier(n_estimators=800,learning_rate=0.1, feature_fraction = 0.9, random_state=seed_model, reg_lambda=10)
lgbm_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_to_use, correct_ind)

# Random Forest
model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
rf_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_to_use, correct_ind)

# Xgboost
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1)
xgb_outcome_layer_2 = K_fold_train_prediction(model, df_train, df_test, seed_skf, 10, feature_to_use, correct_ind)

###
model_train_outcome_list = [rf_outcome_layer_2[0],
                            xgb_outcome_layer_2[0],
                            lgbm_outcome_layer_2[0]]
model_train_name_list = ['Random_Forest_v1',
                         'Xbgoost_v1',
                         'LGBM_v1']

model_test_outcome_list = [rf_outcome_layer_2[1],
                            xgb_outcome_layer_2[1],
                            lgbm_outcome_layer_2[1]]

np.c_[model_train_outcome_list].transpose()

pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list).corr()

X_train_layer_2_df  = pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list)
X_train_layer_2_df['Target'] = df_train['Target']

X_test_layer_2 = np.c_[model_test_outcome_list].transpose()

###
model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1) # 0.95563250049667781
model = LogisticRegression() # 0.95241821254937364
model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1) # 0.95235685047434926
model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 4, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1) # 0.95563992185405167
model = LGBMClassifier(learning_rate = 0.03,
            max_depth = 20,
            scale_pos_weight = 400,
            num_leaves = 30, 
            num_trees = 500, 
            objective = 'binary', 
            lambda_l2 = 100,
            metric = 'auc',
                      random_state = seed_model,
                      n_jobs = -1,
                         n_estimators = 500) # 0.95448216126795526
model = XGBClassifier(n_estimators = 500,
                          random_state = seed_model,
                          n_jobs = -1) # 0.95511344473133963
model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 5, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1) # 0.95557295379856944
model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 5, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1) # 0.95557295379856944
model = LogisticRegressionCV(random_state = seed_model, n_jobs = -1)
model = LGBMClassifier(n_estimators=600,learning_rate=0.1, feature_fraction = 0.9, random_state=seed_model, reg_lambda=10) # 
###

K_fold_validation(model, 10, X_train_layer_2_df, model_train_name_list, seed_skf)

model.fit(X_train_layer_2_df.ix[:, :-1].values, X_train_layer_2_df['Target'].values)



model.fit(df_train.ix[:, 1:].values, df_train['Target'].values)
model.predict_proba(df_test.ix[:, feature_to_use].values)[:, 1]

df_test = pd.read_csv('df_test.csv')
df_submit = pd.concat([df_test['FileID'], pd.Series(model.predict_proba(X_test_layer_2)[:, 1])], axis = 1)
df_submit.to_csv('./Submit_stacking_v5.csv', index = False, header = False)

df_submit = pd.concat([df_test['FileID'], pd.Series(model.predict_proba(df_test.ix[:, feature_to_use].values)[:, 1])], axis = 1)
df_submit.to_csv('./Submit_lgbm_base_v8.csv', index = False, header = False)


### 
model_train_outcome_list = [rf_outcome_layer_2[0],
                            xgb_outcome_layer_2[0],
                            lgbm_outcome_layer_2[0],
                            ada_outcome_layer_2[0],
                            gradient_outcome_layer_2[0],
                            extratrees_outcome_layer_2[0]]
model_train_name_list = ['Random_Forest_v1',
                         'Xbgoost_v1',
                         'LGBM_v1',
                         'Adaboost_v1',
                         'Gradient_Boosting_v1',
                         'Extra_Trees_v1']

model_test_outcome_list = [rf_outcome_layer_2[1],
                            xgb_outcome_layer_2[1],
                            lgbm_outcome_layer_2[1],
                            ada_outcome_layer_2[1],
                            gradient_outcome_layer_2[1],
                            extratrees_outcome_layer_2[1]]

np.c_[model_train_outcome_list].transpose()

pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list).corr()

X_train_layer_2_df  = pd.DataFrame(np.c_[model_train_outcome_list].transpose(), columns = model_train_name_list)
X_train_layer_2_df['Target'] = df_train['Target']

X_test_layer_2 = np.c_[model_test_outcome_list].transpose()


### 
model = LogisticRegression() 
# 0.95165463307364084

model = LogisticRegressionCV(random_state = seed_model)
# 0.95178584602239924

model = XGBClassifier(n_estimators = 300,
                          random_state = seed_model,
                          n_jobs = -1) 
# 0.95310455926520488

model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)
#  0.94756427512588814

model = LGBMClassifier(n_estimators = 300,
                      random_state = seed_model,
                      n_jobs = -1)
# 0.95088487370942187

model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 4, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1)
# 0.9538348559727734

model = LGBMClassifier(learning_rate = 0.03,
            max_depth = 20,
            scale_pos_weight = 400,
            num_leaves = 30, 
            num_trees = 500, 
            objective = 'binary', 
            lambda_l2 = 100,
            metric = 'auc',
                      random_state = seed_model,
                      n_jobs = -1,
                         n_estimators = 500)
# 0.95167506634025634


K_fold_validation(model, 10, X_train_layer_2_df, model_train_name_list, seed_skf)


model = LogisticRegression()
model = XGBClassifier(n_estimators=100,
        eta= 0.1, 
          max_depth= 4, 
          subsample= 0.9, 
          colsample_bytree= 0.7, 
          colsample_bylevel=0.7,
          min_child_weight=100,
          alpha=4,
          objective= 'binary:logistic', 
          eval_metric= 'auc',
                          random_state = seed_model,
                          n_jobs = -1)
model = LogisticRegressionCV(random_state = seed_model)
model.fit(X_train_layer_2_df.ix[:, :-1].values, X_train_layer_2_df['Target'].values)
model.feature_importances_
model.coef_

roc_auc_score(X_train_layer_2_df['Target'].values, model.predict_proba(X_train_layer_2_df.ix[:, :-1].values)[:, 1])

model.predict_proba(X_test_layer_2)[:, 1]

df_test = pd.read_csv('df_test.csv')
df_submit = pd.concat([df_test['FileID'], pd.Series(model.predict_proba(X_test_layer_2)[:, 1])], axis = 1)
df_submit.to_csv('./Submit_stacking_v3.csv', index = False, header = False)
#####################################################################################################################################





























skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1000)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=seed_model)
    rf.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    rf.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)


###
### Random Forest
###

### Feature Importance
rf_model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_feature,
                                  n_jobs = -1)

start_time = time()
rf_model.fit(X_train, y_train) # 61.356860637664795s
print(time() - start_time)

rf_feature_importance = sorted(list(zip(list(df_tr.columns)[3:], 
               rf_model.feature_importances_)), key = lambda x: x[1]*(-1))

with open('./Feature_sets/RandomForest_feature_importance', 'wb') as fp:
    pickle.dump(rf_feature_importance, fp)

roc_auc_score(y_train, rf_model.predict_proba(X_train)[:, 1])

feature_to_use = [x[0] for x in rf_feature_importance][:50]

### Modeling test
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1000)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=seed_model)
    rf.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    rf.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Top 100: 0.93815039835754577
# 461.5080738067627s
'''
0.942274390546
0.934424271522
0.939249667406
0.92449135255
0.950401773836
0.932956984479
0.936269327421
0.940861197339
0.941194087214
0.939380931264
'''
# Top 60: 0.9387919518143123
# 343.79852080345154
'''
0.941806078851
0.937802426728
0.937613895048
0.925750776053
0.949242572062
0.933648189209
0.937158906135
0.942776940133
0.943704951959
0.938414781966
'''
# Top 50: 0.93892735915610392
# 332.5502305030823
'''
0.944271793009
0.936508376527
0.938946341463
0.922392017738
0.951178713969
0.937100665188
0.935436511456
0.940320177384
0.942681744272
0.940437250554
'''
# Top 45: 0.93913875208911379
# 258.50890707969666
'''
0.945865173417
0.937965215176
0.937403399852
0.926233555063
0.949111603843
0.934219660015
0.936527716186
0.941955949741
0.941764375462
0.940340872136
'''
# Top 40: 0.93920042422320493
# 248.14970016479492
'''
0.945313992965
0.936843390435
0.937847450111
0.924583296378
0.950643015521
0.935513968958
0.939118107908
0.940042276423
0.943259127864
0.938839615669
'''
# Top 30: 0.93786099979804427
# 246.20231556892395s
'''
0.943446054503
0.934907328547
0.936022468588
0.928284996305
0.946653954176
0.93289578714
0.94046504065
0.936392609017
0.940299186992
0.939242572062
'''

# The top 40 features set is the best


###
### Xgboost
###

### Feature Importance
xgb_model = XGBClassifier(n_estimators = 150,
                          random_state = seed_model,
                          n_jobs = -1)

start_time = time()
xgb_model.fit(X_train, y_train) # 43.52124571800232s
print(time() - start_time)

xgb_feature_importance = sorted(list(zip(list(df_tr.columns)[3:], 
               xgb_model.feature_importances_)), key = lambda x: x[1]*(-1))

with open('./Feature_sets/XGB_feature_importance', 'wb') as fp:
    pickle.dump(xgb_feature_importance, fp)

roc_auc_score(y_train, xgb_model.predict_proba(X_train)[:, 1])

feature_to_use = [x[0] for x in xgb_feature_importance][:80]


### Modeling test
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    xgb = XGBClassifier(n_estimators = 150,
                          random_state = seed_model,
                          n_jobs = -1)
    xgb.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    xgb.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Top 100: 0.92078851886814417
# 415.91943430900574
'''
0.924295704036
0.920581650203
0.924882187731
0.903988174427
0.930939837398
0.91475654102
0.923893569845
0.920338802661
0.923950332594
0.920258388766
'''
# Top 80: 0.92183374636715809
# 288.0891811847687
'''
0.925618065271
0.919875643781
0.925607095344
0.905484700665
0.932400295639
0.916122985957
0.922781374723
0.920868588322
0.925161271249
0.92441744272
'''
# Top 70: 0.92204103414301175
#247.57588696479797
'''
0.926214956248
0.920289102847
0.927048632668
0.905612416851
0.932111751663
0.91629150037
0.925702291205
0.92111751663
0.922139837398
0.923882335551
'''
# Top 60: 0.92193401324699398
# 229.54012203216553
'''
0.924881388562
0.918526740952
0.926222616408
0.906535994087
0.930729342203
0.915426459719
0.927019068736
0.921924020695
0.925054249815
0.923020251293
'''
# Top 50: 0.9216543144058148
# 208.47998189926147
'''
0.925511309079
0.919421487603
0.924194530673
0.90476511456
0.93132594235
0.912073318551
0.927377383592
0.923590835181
0.924494900222
0.923788322247
'''
# Top 40: 0.92060856762565313
# 150.4884581565857
'''
0.925958977311
0.916996765464
0.924538063562
0.90173481153
0.931706725795
0.910822172949
0.925115742794
0.923263858093
0.923225424982
0.922723133777
'''

# The top 70 features set is the best


###
### LGBM
###

### Feature Importance
lgbm_model = LGBMClassifier(n_estimators = 150,
                          random_state = seed_feature,
                          n_jobs = -1)

start_time = time()
lgbm_model.fit(X_train, y_train) # 3.8240890502929688s
print(time() - start_time)

lgbm_feature_importance = sorted(list(zip(list(df_tr.columns)[3:], 
               lgbm_model.feature_importances_)), key = lambda x: x[1]*(-1))

with open('./Feature_sets/LGBM_feature_importance', 'wb') as fp:
    pickle.dump(lgbm_feature_importance, fp)

roc_auc_score(y_train, lgbm_model.predict_proba(X_train)[:, 1])

feature_to_use = [x[0] for x in lgbm_feature_importance][:80]


### Modeling test
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    lgbm = LGBMClassifier(n_estimators = 500,
                          random_state = seed_model,
                          n_jobs = -1)
    lgbm.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    lgbm.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Basic model
# Top 100: 0.94205195800859798
# 66.12122797966003s
# Top 95: 0.94398371876140597
# 63.098583459854126s
# Top 90: 0.94471967622764441
# 61.95754146575928s
# Top 80: 0.94326448355753223
# 57.944555044174194s
# Top 50: 0.94249360003632088
# 33.86965370178223s

# The top 90 features set is the best

lgbm = LGBMClassifier(n_estimators=500,learning_rate=0.08,
                            feature_fraction = 0.9, random_state=seed_model,
                            reg_lambda=5)

feature_to_use = [x[0] for x in lgbm_feature_importance][:50]

skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    lgbm.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    lgbm.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Top 50: 0.94863399447309538
# 85.21851658821106

lgbm = LGBMClassifier(n_estimators=800,learning_rate=0.08,
                            feature_fraction = 0.9, random_state=seed_model,
                            reg_lambda=5)

feature_to_use = [x[0] for x in lgbm_feature_importance][:50]

skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf)

start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(df_tr.ix[:, feature_to_use].values, df_tr['Target'].values)):
    lgbm.fit(df_tr.ix[:, feature_to_use].values[train_ind, :], df_tr['Target'].values[train_ind])
    score = roc_auc_score(df_tr['Target'].values[test_ind],
                                    lgbm.predict_proba(df_tr.ix[:, feature_to_use].values[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Top 50: 0.94930120443541843
# 130.14443492889404












### Making prediction on Multiple Folds
from scipy.stats import pearsonr


def K_fold_train_prediction(model, df_train, df_test, seed_skf, num_fold, feature_to_use):
    
    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=seed_skf)
    
    train_pred_list = []
    test_pred_list = []
    index_list = []
    for train_ind, test_ind in list(skf.split(df_train.ix[:, feature_to_use].values, df_train['Target'].values)):
        start_time = time()
        model.fit(df_train.ix[:, feature_to_use].values[train_ind, :], df_train['Target'].values[train_ind])
        train_pred_list.append(model.predict_proba(df_train.ix[:, feature_to_use].values[test_ind, :])[:, 1])
        test_pred_list.append(model.predict_proba(df_test.ix[:, feature_to_use].values)[:, 1])
        index_list.append(test_ind)
        print(time() - start_time)
    
    return [np.concatenate(train_pred_list),
            np.mean(np.c_[test_pred_list].transpose(), axis = 1),
            np.concatenate(index_list)]

##
lgbm_model = LGBMClassifier(n_estimators = 150,
                          random_state = seed_model,
                          n_jobs = -1)

lgbm_model = LGBMClassifier(n_estimators=800,learning_rate=0.08,
                            feature_fraction = 0.9, random_state=seed_model,
                            reg_lambda=5)

lgbm_feature_to_use = [x[0] for x in lgbm_feature_importance][:90]
lgbm_outcome = K_fold_train_prediction(lgbm_model, df_tr, df_valid, seed_skf, 10, lgbm_feature_to_use)



##
rf_model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)

rf_feature_to_use = [x[0] for x in rf_feature_importance][:40]
rf_outcome = K_fold_train_prediction(rf_model, df_tr, df_valid, seed_skf, 10, rf_feature_to_use)

##
xgb_model = XGBClassifier(n_estimators = 150,
                          random_state = seed_model,
                          n_jobs = -1)

xgb_feature_to_use = [x[0] for x in xgb_feature_importance][:70]
xgb_outcome = K_fold_train_prediction(xgb_model, df_tr, df_valid, seed_skf, 10, xgb_feature_to_use)

layer_2_ind = lgbm_outcome[2]

## Correlation
pd.DataFrame(np.c_[lgbm_outcome[0], rf_outcome[0], xgb_outcome[0]]).corr()


### Modeling on the Ensemble data
## Layer 2 Data
X_train_layer_2 = pd.DataFrame(np.c_[lgbm_outcome[0], rf_outcome[0], xgb_outcome[0]]).values
y_train_layer_2 = df_tr['Target'].values[layer_2_ind]

## Modeling
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_skf_layer_2)

model = LogisticRegression()
model = LGBMClassifier(n_estimators = 150,
                          random_state = seed_model,
                          n_jobs = -1)
model = RandomForestClassifier(n_estimators = 500,
                                  random_state = seed_model,
                                  n_jobs = -1)


start_time = time()
score_list = []
for train_ind, test_ind in list(skf.split(X_train_layer_2, y_train_layer_2)):
    model.fit(X_train_layer_2[train_ind, :], y_train_layer_2[train_ind])
    score = roc_auc_score(y_train_layer_2[test_ind],
                                    model.predict_proba(X_train_layer_2[test_ind, :])[:, 1])
    score_list.append(score)
    print(score)
done_time = time() - start_time
print(done_time)

np.mean(score_list)

# Logistic Regression: 0.94375601903276407
# LGBM: 0.94488522788186535
# Random Forest: 0.93446254681692786


### Making Prediction on the Validation Set
X_valid_layer_2 = pd.DataFrame(np.c_[lgbm_outcome[1], rf_outcome[1], xgb_outcome[1]]).values
y_valid_layer_2 = df_valid['Target'].values

roc_auc_score(y_valid_layer_2,
              model.predict_proba(X_valid_layer_2)[:, 1])














### Define the Metrics

# AUC for a binary classifier  
def auc(y_true, y_pred):
     auc = tf.metrics.auc(y_true, y_pred)[1]
     K.get_session().run(tf.local_variables_initializer())
     return auc
#-----------------------------------------------------------------------------------------------------------------------------------------------------  
# PFA, prob false alert for binary classifier  
def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):  
    y_pred = K.cast(y_pred >= threshold, 'float32')  
    # N = total number of negative labels  
    N = K.sum(1 - y_true)  
    # FP = total number of false alerts, alerts from the negative class labels  
    FP = K.sum(y_pred - y_pred * y_true)  
    return FP/N  
#-----------------------------------------------------------------------------------------------------------------------------------------------------  
# P_TA prob true alerts for binary classifier  
def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):  
    y_pred = K.cast(y_pred >= threshold, 'float32')  
    # P = total number of positive labels  
    P = K.sum(y_true)  
    # TP = total number of correct alerts, alerts from the positive class labels  
    TP = K.sum(y_pred * y_true)  
    return TP/P  


### Build the Model
input_shape = X_train.shape[1]    
valid_ratio = 0.2

## Create the Model
from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Dense(256, input_dim=input_shape, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(8, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))
model.summary()

## Compile the Model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])

## Fit the Model
model.fit(X_train, y_train, validation_split=valid_ratio, epochs=150, batch_size=32)


















